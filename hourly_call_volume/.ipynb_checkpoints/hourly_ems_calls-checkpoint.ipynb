{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides the framework for doing the batch transform for hourly call volume predictions using NFORS data. The cells at the beginning of the notebook are all required for running the code, but the end of the notebook contains the code I used for creating the datasets for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If running in local mode, local to True. Otherwise, set it to false\n",
    "local = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Sagemaker role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# S3 prefix\n",
    "prefix = 'hourly_call_volume'\n",
    "\n",
    "\n",
    "if local == True:\n",
    "    sagemaker_session = sagemaker.LocalSession()\n",
    "    role = 'arn:aws:iam::445861113736:role/service-role/AmazonSageMaker-ExecutionRole-20190903T114521'\n",
    "else:\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SageMaker Scikit Estimator <a class=\"anchor\" id=\"create_sklearn_estimator\"></a>\n",
    "\n",
    "To run our Scikit-learn training script on SageMaker, we construct a `sagemaker.sklearn.estimator.sklearn` estimator, which accepts several constructor arguments:\n",
    "\n",
    "* __entry_point__: The path to the Python script SageMaker runs for training and prediction.\n",
    "* __role__: Role ARN\n",
    "* __train_instance_type__ *(optional)*: The type of SageMaker instances for training. __Note__: Because Scikit-learn does not natively support GPU training, Sagemaker Scikit-learn does not currently support training on GPU instance types.\n",
    "* __sagemaker_session__ *(optional)*: The session used to train on Sagemaker.\n",
    "* __hyperparameters__ *(optional)*: A dictionary passed to the train function as hyperparameters.\n",
    "\n",
    "To see the code for the SKLearn Estimator, see here: https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "script_path = 'hourly_call_prediction.py'\n",
    "sklearn = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    train_instance_type=\"ml.c4.xlarge\",\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters={'n_estimators': 1000})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SKLearn Estimator on call volume data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-23 17:25:33 Starting - Starting the training job..."
     ]
    }
   ],
   "source": [
    "#The data should already be saved to the ./data directory\n",
    "WORK_DIRECTORY = 'data'\n",
    "train_input = sagemaker_session.upload_data(WORK_DIRECTORY, key_prefix=\"{}/{}\".format(prefix, WORK_DIRECTORY) )\n",
    "\n",
    "#Training the model\n",
    "sklearn.fit({'train': train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Transform <a class=\"anchor\" id=\"batch_transform\"></a>\n",
    "We can also use the trained model for asynchronous batch inference on S3 data using SageMaker Batch Transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a SKLearn Transformer from the trained SKLearn Estimator\n",
    "transformer = sklearn.transformer(instance_count=1, instance_type='ml.m4.xlarge', assemble_with = 'Line', accept = 'text/csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Transform Job <a class=\"anchor\" id=\"run_transform_job\"></a>\n",
    "Using the Transformer, run a transform job on the S3 input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again, the test data should already be saved to the prediction_data directory\n",
    "WORK_DIRECTORY = 'prediction_data/test_data'\n",
    "batch_input_s3 = sagemaker_session.upload_data(WORK_DIRECTORY, key_prefix=\"{}/{}\".format(prefix, WORK_DIRECTORY) )\n",
    "\n",
    "# Start a transform job and wait for it to finish\n",
    "transformer.transform(batch_input_s3, content_type='text/csv', split_type='Line',   join_source='Input')\n",
    "print('Waiting for transform job: ' + transformer.latest_transform_job.job_name)\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dowloading batch transform results from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Download the output data from S3 to local filesystem\n",
    "batch_output = transformer.output_path\n",
    "!mkdir -p batch_data/output\n",
    "!aws s3 cp --recursive $batch_output/ batch_data/output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-dark')\n",
    "\n",
    "\n",
    "#Loading the data\n",
    "results = pd.read_csv('./batch_data/output/test_data.out', names = ['fire_department.firecares_id',\n",
    "                                                          'description.day_of_week','hour', 'call_volume'], header=None,\n",
    "                                                         dtype={'hour': object, 'fire_department.firecares_id': object})\n",
    "\n",
    "fc_id = '93345'\n",
    "\n",
    "\n",
    "\n",
    "for i,day in enumerate(results['description.day_of_week'].unique()):\n",
    "    print\n",
    "    subset = results[ (results['fire_department.firecares_id'] == fc_id) & (results['description.day_of_week'] == day)]\n",
    "    subset.plot.bar('hour', 'call_volume')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the training data and uploading to s3\n",
    "This cell only works locally because it depends on elasticsearch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries required for performing the query\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search\n",
    "from elasticsearch_dsl import Q\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "#Setting up the query\n",
    "es = Elasticsearch()\n",
    "s = Search(using=es,index='*-fire-incident-*')\n",
    "s = s.source(['description.event_opened',\n",
    "                     'description.day_of_week',\n",
    "                    'NFPA.type',\n",
    "                     'fire_department.firecares_id'])\n",
    "\n",
    "\n",
    "#Performing the query\n",
    "q = Q(\"match\",fire_department__firecares_id =  '79592') | Q(\"match\",fire_department__firecares_id =  '93345')\n",
    "results = s.query(q)\n",
    "\n",
    "#Converting query results to a pandas dataframe\n",
    "df = pd.DataFrame((d.to_dict() for d in tqdm_notebook(results.scan())))\n",
    "json_struct = json.loads(df.to_json(orient=\"records\"))\n",
    "\n",
    "df = pd.io.json.json_normalize(json_struct)\n",
    "\n",
    "#Converting date\n",
    "df['date'] = df['description.event_opened'].apply(lambda x: x[:10])\n",
    "df['month'] = df.apply(lambda x: x['date'][5:7], axis=1)\n",
    "df['hour'] = df['description.event_opened'].apply(lambda x: x[11:13])\n",
    "\n",
    "\n",
    "#Converting df dates to datetime objects\n",
    "df['date'] = df.apply(lambda x: datetime.datetime.strptime(x['date'],'%Y-%m-%d'),axis=1)\n",
    "# df['date'] = df.apply(lambda x: datetime.datetime.strptime(x['date'],'%Y-%m-%d'),axis=1)\n",
    "\n",
    "#It's convenient to serialize (pickle) the dataframe because it's faster to load it rather than re-create it.\n",
    "df.to_pickle('query_results')\n",
    "\n",
    "#Hourly is a dataframe aggregated grouped by the day, hour, and department\n",
    "hourly = df[['fire_department.firecares_id', 'date','description.day_of_week', 'hour']].groupby(['fire_department.firecares_id', 'date','description.day_of_week', 'hour']).aggregate(len).reset_index()\n",
    "hourly = hourly.rename(columns={0: 'calls'})\n",
    "\n",
    "#Formatting the hourly dataframe into a json\n",
    "jsondata = {}\n",
    "jsondata['model_name'] = 'calls_by_hour'\n",
    "jsondata['model_version'] = 1.0\n",
    "jsondata['prediction_data'] = hourly.drop('date',axis=1).to_dict(orient='records')\n",
    "\n",
    "#Saving the json to the data directory\n",
    "with open('./data/training_data.json', 'w') as outfile:\n",
    "    json.dump(jsondata, outfile)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the test dataset\n",
    "This involves generating a dataframe with every possible combination of department, day of week, and hour of the day. The size of this dataset is 7x24xn, where n is the number of departments included for predictions.\n",
    "\n",
    "Also, this cell requires that the training set has already been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make every combination of departments, days of week, and hour\n",
    "from itertools import product\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "#load the training dataset\n",
    "with open('./data/training_data.json') as data_file:\n",
    "    data = json.load(data_file)\n",
    "hourly = pd.io.json.json_normalize(data['prediction_data'])\n",
    "\n",
    "#Getting the list of ever department that shows up in the hourly dataframe\n",
    "dep_list = hourly['fire_department.firecares_id'].unique()\n",
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "hours = [str(i).zfill(2) for i in range(24)]\n",
    "\n",
    "#Creating the dataframe of all possible combinations\n",
    "test_df = pd.DataFrame(list(product(dep_list, days, hours)), columns=['fire_department.firecares_id', 'description.day_of_week', 'hour'])\n",
    "\n",
    "#Saving it as a csv\n",
    "test_df.to_csv('./prediction_data/test_data', index=False, header=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p27",
   "language": "python",
   "name": "conda_amazonei_mxnet_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
