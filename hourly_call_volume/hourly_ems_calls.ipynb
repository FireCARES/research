{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides the framework for doing the batch transform for hourly call volume predictions using NFORS data. The cells at the beginning of the notebook are all required for running the code, but the end of the notebook contains the code I used for creating the datasets for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If running in local mode, local to True. Otherwise, set it to false\n",
    "local = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Sagemaker role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# S3 prefix\n",
    "prefix = 'hourly_call_volume'\n",
    "\n",
    "\n",
    "if local == True:\n",
    "    sagemaker_session = sagemaker.LocalSession()\n",
    "    role = 'arn:aws:iam::445861113736:role/service-role/AmazonSageMaker-ExecutionRole-20190903T114521'\n",
    "else:\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SageMaker Scikit Estimator <a class=\"anchor\" id=\"create_sklearn_estimator\"></a>\n",
    "\n",
    "To run our Scikit-learn training script on SageMaker, we construct a `sagemaker.sklearn.estimator.sklearn` estimator, which accepts several constructor arguments:\n",
    "\n",
    "* __entry_point__: The path to the Python script SageMaker runs for training and prediction.\n",
    "* __role__: Role ARN\n",
    "* __train_instance_type__ *(optional)*: The type of SageMaker instances for training. __Note__: Because Scikit-learn does not natively support GPU training, Sagemaker Scikit-learn does not currently support training on GPU instance types.\n",
    "* __sagemaker_session__ *(optional)*: The session used to train on Sagemaker.\n",
    "* __hyperparameters__ *(optional)*: A dictionary passed to the train function as hyperparameters.\n",
    "\n",
    "To see the code for the SKLearn Estimator, see here: https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "script_path = 'hourly_call_prediction.py'\n",
    "sklearn = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    train_instance_type=\"ml.c4.xlarge\",\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters={'n_estimators': 1000})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SKLearn Estimator on call volume data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-23 17:25:33 Starting - Starting the training job...\n",
      "2020-01-23 17:25:35 Starting - Launching requested ML instances...\n",
      "2020-01-23 17:26:31 Starting - Preparing the instances for training......\n",
      "2020-01-23 17:27:28 Downloading - Downloading input data...\n",
      "2020-01-23 17:27:58 Training - Downloading the training image..\u001b[34m2020-01-23 17:28:12,311 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2020-01-23 17:28:12,314 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-01-23 17:28:12,327 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-01-23 17:28:12,611 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-01-23 17:28:12,611 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-01-23 17:28:12,611 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-01-23 17:28:12,612 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmpp6kchyub/module_dir\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=7422 sha256=26e983c7d31e2f900014a12190f5aed74670cb967dc24dde849e675b30ab7b8f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1drqyhie/wheels/60/43/42/9319fa1bd596a07e75f5e71a1f13375a92fe1ceaaa3888a805\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[34mInstalling collected packages: default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0\u001b[0m\n",
      "\u001b[34m2020-01-23 17:28:14,125 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-01-23 17:28:15,566 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-01-23 17:28:15,580 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-01-23 17:28:15,590 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"n_estimators\": 1000\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-scikit-learn-2020-01-23-17-25-32-937\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-445861113736/sagemaker-scikit-learn-2020-01-23-17-25-32-937/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"hourly_call_prediction\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"hourly_call_prediction.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"n_estimators\":1000}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=hourly_call_prediction.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=hourly_call_prediction\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-445861113736/sagemaker-scikit-learn-2020-01-23-17-25-32-937/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"n_estimators\":1000},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-scikit-learn-2020-01-23-17-25-32-937\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-445861113736/sagemaker-scikit-learn-2020-01-23-17-25-32-937/source/sourcedir.tar.gz\",\"module_name\":\"hourly_call_prediction\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"hourly_call_prediction.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--n_estimators\",\"1000\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_N_ESTIMATORS=1000\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python hourly_call_prediction.py --n_estimators 1000\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\n",
      "2020-01-23 17:28:11 Training - Training image download completed. Training in progress.\n",
      "2020-01-23 17:29:32 Uploading - Uploading generated training model\u001b[34m2020-01-23 17:29:30,331 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-01-23 17:29:39 Completed - Training job completed\n",
      "Training seconds: 131\n",
      "Billable seconds: 131\n"
     ]
    }
   ],
   "source": [
    "#The data should already be saved to the ./data directory\n",
    "WORK_DIRECTORY = 'data'\n",
    "train_input = sagemaker_session.upload_data(WORK_DIRECTORY, key_prefix=\"{}/{}\".format(prefix, WORK_DIRECTORY) )\n",
    "\n",
    "#Training the model\n",
    "sklearn.fit({'train': train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Transform <a class=\"anchor\" id=\"batch_transform\"></a>\n",
    "We can also use the trained model for asynchronous batch inference on S3 data using SageMaker Batch Transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a SKLearn Transformer from the trained SKLearn Estimator\n",
    "transformer = sklearn.transformer(instance_count=1, instance_type='ml.m4.xlarge', assemble_with = 'Line', accept = 'text/csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Transform Job <a class=\"anchor\" id=\"run_transform_job\"></a>\n",
    "Using the Transformer, run a transform job on the S3 input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for transform job: sagemaker-scikit-learn-2020-01-23-17-30-16-229\n",
      ".....................\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: hourly-call-prediction\n",
      "  Building wheel for hourly-call-prediction (setup.py): started\n",
      "  Building wheel for hourly-call-prediction (setup.py): finished with status 'done'\n",
      "  Created wheel for hourly-call-prediction: filename=hourly_call_prediction-1.0.0-py2.py3-none-any.whl size=7325 sha256=e28746f659a6a67925f1c225e0730a0341ce292ec585ac17bfdfcb2a1e7e7212\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0avk1xqz/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built hourly-call-prediction\u001b[0m\n",
      "\u001b[34mInstalling collected packages: hourly-call-prediction\u001b[0m\n",
      "\u001b[34mSuccessfully installed hourly-call-prediction-1.0.0\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m[2020-01-23 17:33:27 +0000] [38] [INFO] Starting gunicorn 20.0.3\u001b[0m\n",
      "\u001b[34m[2020-01-23 17:33:27 +0000] [38] [INFO] Listening at: unix:/tmp/gunicorn.sock (38)\u001b[0m\n",
      "\u001b[34m[2020-01-23 17:33:27 +0000] [38] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2020-01-23 17:33:27 +0000] [42] [INFO] Booting worker with pid: 42\u001b[0m\n",
      "\u001b[34m[2020-01-23 17:33:27 +0000] [43] [INFO] Booting worker with pid: 43\u001b[0m\n",
      "\u001b[34m[2020-01-23 17:33:27 +0000] [47] [INFO] Booting worker with pid: 47\u001b[0m\n",
      "\u001b[34m[2020-01-23 17:33:27 +0000] [48] [INFO] Booting worker with pid: 48\u001b[0m\n",
      "\u001b[34m2020-01-23 17:33:45,990 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2020-01-23 17:33:45,990 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [23/Jan/2020:17:33:46 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m2020-01-23 17:33:46,953 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [23/Jan/2020:17:33:46 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m2020-01-23 17:33:46,953 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [23/Jan/2020:17:33:47 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m2020-01-23 17:33:47,840 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [23/Jan/2020:17:33:47 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m2020-01-23 17:33:47,840 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [23/Jan/2020:17:33:48 +0000] \"POST /invocations HTTP/1.1\" 200 6326 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [23/Jan/2020:17:33:48 +0000] \"POST /invocations HTTP/1.1\" 200 6326 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2020-01-23T17:33:47.804:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Again, the test data should already be saved to the prediction_data directory\n",
    "WORK_DIRECTORY = 'prediction_data/test_data'\n",
    "batch_input_s3 = sagemaker_session.upload_data(WORK_DIRECTORY, key_prefix=\"{}/{}\".format(prefix, WORK_DIRECTORY) )\n",
    "\n",
    "# Start a transform job and wait for it to finish\n",
    "transformer.transform(batch_input_s3, content_type='text/csv', split_type='Line',   join_source='Input')\n",
    "print('Waiting for transform job: ' + transformer.latest_transform_job.job_name)\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dowloading batch transform results from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 11.8 KiB/11.8 KiB (184.6 KiB/s) with 1 file(s) remaining\r",
      "download: s3://sagemaker-us-east-2-445861113736/sagemaker-scikit-learn-2020-01-23-17-30-16-229/test_data.out to batch_data/output/test_data.out\r\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Download the output data from S3 to local filesystem\n",
    "batch_output = transformer.output_path\n",
    "!mkdir -p batch_data/output\n",
    "!aws s3 cp --recursive $batch_output/ batch_data/output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'impo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5a8c114e77f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimpo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#Loading the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m results = pd.read_csv('./batch_data/output/test_data.out', names = ['fire_department.firecares_id',\n\u001b[1;32m      5\u001b[0m                                                           'description.day_of_week','hour', 'call_volume'], header=None,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'impo' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-dark')\n",
    "\n",
    "\n",
    "#Loading the data\n",
    "results = pd.read_csv('./batch_data/output/test_data.out', names = ['fire_department.firecares_id',\n",
    "                                                          'description.day_of_week','hour', 'call_volume'], header=None,\n",
    "                                                         dtype={'hour': object, 'fire_department.firecares_id': object})\n",
    "\n",
    "fc_id = '93345'\n",
    "\n",
    "\n",
    "\n",
    "for i,day in enumerate(results['description.day_of_week'].unique()):\n",
    "    print\n",
    "    subset = results[ (results['fire_department.firecares_id'] == fc_id) & (results['description.day_of_week'] == day)]\n",
    "    subset.plot.bar('hour', 'call_volume')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the training data and uploading to s3\n",
    "This cell only works locally because it depends on elasticsearch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries required for performing the query\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search\n",
    "from elasticsearch_dsl import Q\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "#Setting up the query\n",
    "es = Elasticsearch()\n",
    "s = Search(using=es,index='*-fire-incident-*')\n",
    "s = s.source(['description.event_opened',\n",
    "                     'description.day_of_week',\n",
    "                    'NFPA.type',\n",
    "                     'fire_department.firecares_id'])\n",
    "\n",
    "\n",
    "#Performing the query\n",
    "q = Q(\"match\",fire_department__firecares_id =  '79592') | Q(\"match\",fire_department__firecares_id =  '93345')\n",
    "results = s.query(q)\n",
    "\n",
    "#Converting query results to a pandas dataframe\n",
    "df = pd.DataFrame((d.to_dict() for d in tqdm_notebook(results.scan())))\n",
    "json_struct = json.loads(df.to_json(orient=\"records\"))\n",
    "\n",
    "df = pd.io.json.json_normalize(json_struct)\n",
    "\n",
    "#Converting date\n",
    "df['date'] = df['description.event_opened'].apply(lambda x: x[:10])\n",
    "df['month'] = df.apply(lambda x: x['date'][5:7], axis=1)\n",
    "df['hour'] = df['description.event_opened'].apply(lambda x: x[11:13])\n",
    "\n",
    "\n",
    "#Converting df dates to datetime objects\n",
    "df['date'] = df.apply(lambda x: datetime.datetime.strptime(x['date'],'%Y-%m-%d'),axis=1)\n",
    "# df['date'] = df.apply(lambda x: datetime.datetime.strptime(x['date'],'%Y-%m-%d'),axis=1)\n",
    "\n",
    "#It's convenient to serialize (pickle) the dataframe because it's faster to load it rather than re-create it.\n",
    "df.to_pickle('query_results')\n",
    "\n",
    "#Hourly is a dataframe aggregated grouped by the day, hour, and department\n",
    "hourly = df[['fire_department.firecares_id', 'date','description.day_of_week', 'hour']].groupby(['fire_department.firecares_id', 'date','description.day_of_week', 'hour']).aggregate(len).reset_index()\n",
    "hourly = hourly.rename(columns={0: 'calls'})\n",
    "\n",
    "#Formatting the hourly dataframe into a json\n",
    "jsondata = {}\n",
    "jsondata['model_name'] = 'calls_by_hour'\n",
    "jsondata['model_version'] = 1.0\n",
    "jsondata['prediction_data'] = hourly.drop('date',axis=1).to_dict(orient='records')\n",
    "\n",
    "#Saving the json to the data directory\n",
    "with open('./data/training_data.json', 'w') as outfile:\n",
    "    json.dump(jsondata, outfile)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the test dataset\n",
    "This involves generating a dataframe with every possible combination of department, day of week, and hour of the day. The size of this dataset is 7x24xn, where n is the number of departments included for predictions.\n",
    "\n",
    "Also, this cell requires that the training set has already been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make every combination of departments, days of week, and hour\n",
    "from itertools import product\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "#load the training dataset\n",
    "with open('./data/training_data.json') as data_file:\n",
    "    data = json.load(data_file)\n",
    "hourly = pd.io.json.json_normalize(data['prediction_data'])\n",
    "\n",
    "#Getting the list of ever department that shows up in the hourly dataframe\n",
    "dep_list = hourly['fire_department.firecares_id'].unique()\n",
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "hours = [str(i).zfill(2) for i in range(24)]\n",
    "\n",
    "#Creating the dataframe of all possible combinations\n",
    "test_df = pd.DataFrame(list(product(dep_list, days, hours)), columns=['fire_department.firecares_id', 'description.day_of_week', 'hour'])\n",
    "\n",
    "#Saving it as a csv\n",
    "test_df.to_csv('./prediction_data/test_data', index=False, header=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p27",
   "language": "python",
   "name": "conda_amazonei_mxnet_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
